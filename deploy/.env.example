NUM_THREADS=4  # num threads to run model per worker
LLM_LIB=avx   # change to basic if encounter core dumped 
LOAD_ON_FLY=False  #  whether or not to load model into ram each time requesting, enable to save a RAM but inference speed will be slower a bit.
{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OASST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"HuggingFaceH4/oasst1_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_mapping = {\n",
    "    \"user\": \"human\",\n",
    "    \"assistant\": \"bot\"\n",
    "}\n",
    "def convert_oassten(row):\n",
    "    data = []\n",
    "    for turn in row[\"messages\"]:\n",
    "        data.append({\n",
    "            \"text\": turn[\"content\"],\n",
    "            \"role\": role_mapping[turn[\"role\"]]\n",
    "        })\n",
    "    return {\"conversations\": data}\n",
    "\n",
    "oassten = ds[\"train_ift\"].map(convert_oassten, num_proc=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dolly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"HuggingFaceH4/databricks_dolly_15k\")\n",
    "train = ds['train']      # len(train)=84437 (95%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_mapping = {\n",
    "    \"user\": \"human\",\n",
    "    \"assistant\": \"bot\"\n",
    "}\n",
    "def convert_dolly(row):\n",
    "    return {\"conversations\": [\n",
    "        {\n",
    "            \"role\": \"human\",\n",
    "            \"text\": f\"{row['instruction']}\\n{row['input']}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"bot\",\n",
    "            \"text\": f\"{row['output']}\"\n",
    "        }\n",
    "    ]}\n",
    "\n",
    "dolly15k = ds['train'].map(convert_dolly, num_proc=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"daily_dialog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "ptn1 = [re.compile(\"\\s+\\?\"), re.compile(\"\\s+\\.\"), re.compile(\"\\s+!\"), re.compile(\"\\s+,\"),\n",
    "        re.compile(\"\\(\\s+\"), re.compile(\"\\s+\\)\"),]\n",
    "ptn2 = re.compile(\"\\s+'\\s+\")\n",
    "ptn3 = re.compile(\"\\.\")\n",
    "ptn4 = re.compile(\"[ ]+\")\n",
    "ptn5 = re.compile(\"\\s+’\\s+\")\n",
    "\n",
    "def replacer1(text):\n",
    "    replace_chars = \"?.!,()\"\n",
    "    for i, ptn in enumerate(ptn1):\n",
    "        text = ptn.sub(replace_chars[i], text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_dd(text):\n",
    "    text = replacer1(text)\n",
    "    text = ptn2.sub(\"'\", text)\n",
    "    text = ptn3.sub(\". \", text)\n",
    "    text = ptn4.sub(\" \", text)\n",
    "    text = ptn5.sub(\"’\", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "print(preprocess_dd(\"\\n\".join(ds[\"train\"][0][\"dialog\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_mapping = {\n",
    "    \"user\": \"human\",\n",
    "    \"assistant\": \"bot\"\n",
    "}\n",
    "def convert_dailydialog(row):\n",
    "    data = []\n",
    "    role = \"human\"\n",
    "    for line in row[\"dialog\"]:\n",
    "        data.append(\n",
    "            {\n",
    "            \"role\": role,\n",
    "            \"text\": preprocess_dd(line.strip())\n",
    "            }\n",
    "        )\n",
    "        if role == \"human\":\n",
    "            role = \"bot\"\n",
    "        else:\n",
    "            role = \"human\"\n",
    "    return {\"conversations\": data}\n",
    "        \n",
    "\n",
    "dailydialog = ds['train'].map(convert_dailydialog, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailydialog[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2600/24 * 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "data = oassten[\"conversations\"] + dolly15k[\"conversations\"]\n",
    "\n",
    "with open(\"data/oassten_dolly.json\", \"w\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(oassten[\"conversations\"]) * 0.95 / 16\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
    "os.environ[\"PATH\"] = \"${CUDA_HOME}/bin:${PATH}\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda/lib64:$LD_LIBRARY_PATH\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/pythia1b4-chat-oasst-dolly\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"models/pythia1b4-chat-oasst-dolly\", device_map='auto', \n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             )\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "a = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_format = \"### Input:\\n{human}\\n\\n### Response:\\n\" \n",
    "# prompt_format = \"### Human: {human}\\n\\n### Assistant: \" \n",
    "prompt_format = \"Human:\\n{human}\" + tokenizer.eos_token + \"\\nAssistant:\\n\"\n",
    "\n",
    "TURN_CONCATENATION_TOKEN = \"\"\n",
    "\n",
    "# text = prompt_format.format(**{\"human\": \"Tôi là một thanh niên 26 tuổi, cao 6 feet, cân nặng 130 pounds. Tôi muốn tăng cân, chủ yếu là cơ bắp. Bạn có thể cho tôi một kế hoạch tập luyện trong cả tuần và một bữa ăn cung cấp cho tôi lượng calo cần thiết?\"})\n",
    "# text = prompt_format.format(**{\"human\": \"tóm tắt đoạn sau: Hơn một triệu tỷ đồng ngân quỹ phải gửi nhà băng cho thấy kém hiệu quả trong sử dụng đồng tiền, kinh tế mất đi động lực tăng trưởng, theo các đại biểu Quốc hội.\"})\n",
    "# text = prompt_format.format(**{\"human\": \"Vũ trụ song song? Có những lý thuyết hay bằng chứng khoa học ủng hộ hay phản đối sự tồn tại của chúng?\"})\n",
    "# text = prompt_format.format(**{\"human\": \"Tôi đã nhận được một lá thư 'Thích và không thích' nói rằng tôi đã tham gia vào việc phỉ báng.\"})\n",
    "# text = prompt_format.format(**{\"human\": \"Chào, anh khỏe không? Có thể cho tôi biết thời tiết hôm nay như nào được không?\"})\n",
    "# text = prompt_format.format(**{\"human\": \"Bạn là giáo viên môn Thị giác máy tính. Bạn phải viết thư giới thiệu cho sinh viên muốn xin bằng tiến sĩ trong công ty thị giác máy tính chuyên sâu. Nhấn mạnh rằng sinh viên đó rất tự chủ.\"})\n",
    "# text = prompt_format.format(**{\"human\": \"Cho tôi gặp giám đốc.\"})\n",
    "# text = prompt_format.format(**{\"human\": \"Tell me 3 main differences between university and college\"})\n",
    "# text = prompt_format.format(**{\"human\": \"大学と専門学校の主な違いを3つ教えてください\"})\n",
    "# text = prompt_format.format(**{\"human\": \"大阪の観光スポット10選\"})\n",
    "# text = prompt_format.format(**{\"human\": \"日本でおすすめの料理は？\"})\n",
    "# text = prompt_format.format(**{\"human\": \"Bạn thích món ăn nào?\"}) + \"\\nTôi thích ăn món ăn Việt Nam.</s>\" + prompt_format.format(**{\"human\": \"Cụ thể hơn đi\"}) \n",
    "# text = prompt_format.format(**{\"human\": \"Python program to crawl reddit\"})\n",
    "# text = prompt_format.format(**{\"human\": \"What food do you like?\"})\n",
    "# text = prompt_format.format(**{\"human\": \"What are you ?\"})\n",
    "# text = prompt_format.format(**{\"human\": \"i havent cried out loud since my loved one is gone, i wanna cry out loud, i wanna feel light, \\n\\nwrite this in poetic way\"})\n",
    "# text = prompt_format.format(**{\"human\": \"Review Titanic movie\"})\n",
    "# text = prompt_format.format(**{\"human\": \"Hãy review phim Titanic\"})\n",
    "# text = prompt_format.format(**{\"human\": \"Sửa lỗi chính tả cho câu sau: dắn hổ mang trườn lên lúi\"})\n",
    "# text = prompt_format.format(**{\"human\": \"Hãy đóng giả làm giáo viên dạy tiếng Anh, bắt đầu bằng việc tự giới thiệu bản thân\"})\n",
    "# text = prompt_format.format(**{\"human\": \"3 sự khác biệt giữa đại học và cao đẳng. Dịch câu trên sang tiếng Anh\"})\n",
    "# text = prompt_format.format(**{\"human\": \"Lên thực đơn cho cho gia đình 4 người trong 1 ngày với các món chay\"})\n",
    "# text = prompt_format.format(**{\"human\": \"Làm bài thơ tặng bạn nữ tên Linh nhân ngày quốc tế thiếu nhi\"})\n",
    "# text = prompt_format.format(**{\"human\": \"giới thiệu bản thân bạn như 1 AI Engineer\"})\n",
    "# text = prompt_format.format(**{\"human\": \"Could you write me a short story about a gnome who has found themselves lost in New York and has no idea what is going on\"})\n",
    "# text = prompt_format.format(**{\"human\": \"\"\"Bộ Thông tin và Truyền thông cho biết việc sửa đổi Luật Viễn thông, trong đó có OTT viễn thông, nhằm đảm bảo an toàn, an ninh mạng và quyền lợi người dùng.\n",
    "# Trình Quốc hội dự án Luật Viễn thông (sửa đổi) sáng 2/6, Bộ trưởng Thông tin và Truyền thông Nguyễn Mạnh Hùng nói trước đây, việc cung cấp dịch vụ viễn thông phải có hạ tầng mạng, và quản lý hạ tầng mạng là quản lý luôn được dịch vụ viễn thông.\n",
    "# Còn ngày nay, trên Internet cũng có thể triển khai dịch vụ viễn thông xuyên biên giới, đặt ra bài toán quản lý phải bảo đảm nguyên tắc bình đẳng giữa các dịch vụ cùng vấn đề an toàn, an ninh. Vì vậy, dự thảo quy định chi tiết về quản lý việc cung cấp và hình thức cấp phép với dịch vụ viễn thông, trong đó có trung tâm dữ liệu, điện toán đám mây để đảm bảo tính linh hoạt, đảm bảo cơ chế khuyến khích dịch vụ mới phát triển.\n",
    "# Tóm tắt đoạn văn trên \"\"\"})\n",
    "# text = prompt_format.format(**{\"human\": \"Who is the 46th president of American?\"})\n",
    "# text = \"what are you?\"\n",
    "text = prompt_format.format(**{\"human\": \"what is your name?\"})\n",
    "# text = prompt_format.format(**{\"human\": \"what is the weather today?\"})\n",
    "# text = prompt_format.format(**{\"human\": \"Give me some information about yourself in less than 20 words\"})\n",
    "# text = prompt_format.format(**{\"human\": \"Give me some paper ideas for AAAI 2023 conferences\"})\n",
    "# text = prompt_format.format(**{\"human\": \"Population of Tokyo 2021\"})\n",
    "\n",
    "print(text)\n",
    "# infer\n",
    "# prompt = \"<human>: tóm tắt đoạn sau: Hơn một triệu tỷ đồng ngân quỹ phải gửi nhà băng cho thấy kém hiệu quả trong sử dụng đồng tiền, kinh tế mất đi động lực tăng trưởng, theo các đại biểu Quốc hội.\\n<bot>:\"\n",
    "inputs = tokenizer(text, return_tensors='pt').to(model.device)\n",
    "input_length = inputs.input_ids.shape[1]\n",
    "model.config\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, max_new_tokens=400, do_sample=True, temperature=0.5, top_k=50, return_dict_in_generate=True, no_repeat_ngram_size=5,\n",
    "        # pad_token_id=tokenizer.pad_token_id,\n",
    "        # bos_token_id=tokenizer.bos_token_id,\n",
    "        # eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "token = outputs.sequences[0, input_length:]\n",
    "output_str = tokenizer.decode(token)\n",
    "\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"Zayt/pythia1b-dedup-oasst-dolly-dailydialog\", token=\"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\")\n",
    "tokenizer.push_to_hub(\"Zayt/pythia1b-dedup-oasst-dolly-dailydialog\", token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python = \"^3.10\"\n",
    "fastapi = \"^0.98.0\"\n",
    "transformers = \"^4.30.2\"\n",
    "ctransformers = \"^0.2.10\"\n",
    "pydantic = \"^1.10.9\"\n",
    "loguru = \"^0.7.0\"\n",
    "gunicorn = \"^20.1.0\"\n",
    "uvicorn = \"^0.23.1\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
